install.packages('nullabor')
diamonds
nd<-sample_n(diamonds,5000,replace=FALSE)
set.seed(3456543)
nd<-sample_n(diamonds,5000,replace=FALSE)
nd$depth<-null_permute(nd$depth)
null_permute
set.seed(3456543)
library(nullabor)
nd<-sample_n(diamonds,5000,replace=FALSE)
nd$depth<-null_permute(nd$depth)
null_permute(nd$depth)
nd%>%mutate(depth=null_permute(depth))
knitr::opts_chunk$set(
comment = "#>",
collapse = TRUE,
fig.width=3, fig.height=3
)
comma <- function(x) format(x, digits = 2, big.mark = ",")
options(digits=3)
library(tidyverse)
library(gridExtra)
library(dplyr)
theme_set(theme_minimal())
set.seed(3456543)
library(nullabor)
nd<-sample_n(diamonds,5000,replace=FALSE)
d<-lineup(null_permute(nd$depth))
set.seed(3456543)
library(nullabor)
nd<-sample_n(diamonds,5000,replace=FALSE)
d<-lineup(null_permute("depth"))
set.seed(3456543)
library(nullabor)
nd<-sample_n(diamonds,5000,replace=FALSE)
d<-lineup(null_permute("depth"),nd)
set.seed(3456543)
library(nullabor)
nd<-sample_n(diamonds,5000,replace=FALSE)
d<-lineup(null_permute("depth"),nd)
ggplot(data=d,aes(x=depth,y=carat))+geom_jitter()+facet_wrap(~.sample)
library(nullabor)
library(lattice)
data(singer)
# creo base de datos singer-gender donde Sopranos y Alto -> F
singer_g <- singer %>%
mutate(
gender = case_when(
voice.part %in% c("Soprano 1", "Soprano 2", "Alto 1", "Alto 2") ~ "F",
TRUE ~ "M"),
height = 2.54 * height) %>%
dplyr::select(gender, height)
head(singer_g)
ggplot(singer_g, aes(x = gender, y = height)) +
geom_jitter(position = position_jitter(width = 0.1, height = 1))
paths <- dir("data/2008-2009", pattern = "LAL", full.names = TRUE)
basket <- purrr::set_names(paths, paths) %>%
map_df(read_csv)
basket_LA <- basket %>%
filter(team == "LAL", type == "3pt", !is.na(x), !is.na(y)) %>% # datos Lakers
mutate(
x = x + runif(length(x), -0.5, 0.5),
y = y + runif(length(y), -0.5, 0.5),
r = sqrt((x - 25) ^ 2 + y ^ 2),  # distancia a canasta
angle = atan2(y, x - 25)) %>%  # ángulo
filter(r > 20 & r < 39) %>% # lanzamientos en el rango típico
dplyr::select(x, y, r, angle)
# guardar datos
write.table(basket_LA, file = "data/basket_LA.csv", sep = ",")
glimpse(basket_LA)
basket_null <- lineup(null_lm(r ~ poly(angle, 2)), basket_LA, n = 10)
ggplot(basket_null, aes(x = angle * 180 / pi, y = r)) +
geom_point(alpha = 0.5, size = 0.8) +
scale_x_continuous("Angle (degrees)",
breaks = c(0, 45, 90, 135, 180),
limits = c(0, 180)) +
facet_wrap(~ .sample, nrow = 2)
fit <- lm(r ~ poly(angle, 2), data = basket_LA)
n <- 10
# matriz de covariables
X <- model.matrix(r ~ poly(angle, 2), basket_LA)
n_obs <- nrow(X)
# 9 simulaciones del modelo, cada una del mismo tamaño que los datos
sims <- rep(X %*% coef(fit), n - 1) + rnorm((n - 1) * n_obs, mean = 0, sd = 1)
# creo una base de datos que incluya datos originales
basket_LA_3 <- data.frame(
angle = rep(basket_LA$angle, n), # el valor de x (ángulo) para cada panel
r = c(basket_LA$r, sims),  # distancias simuladas y observadas
id = rep(sample(1:n, size = n), each = n_obs)) # id aleatorio para esconder los datos
ggplot(basket_LA_3, aes(x = angle * 180 / pi, y = r)) +
geom_point(alpha = 0.5, size = 0.8) +
scale_x_continuous("Angle (degrees)",
breaks = c(0, 45, 90, 135, 180),
limits = c(0, 180)) +
facet_wrap(~ id, nrow = 2)
basket_LA$resid <- resid(fit)
basket_null_2 <- lineup(
null_dist("resid", "normal", list(mean = 0, sd = 1)),
basket_LA, n = 10)
ggplot(basket_null_2, aes(x = angle * 180 / pi, y = resid)) +
geom_point(alpha = 0.6, size = 0.8) +
scale_x_continuous("Angle (degrees)",
breaks = c(0, 45, 90, 135, 180),
limits = c(0, 180)) +
facet_wrap(~ .sample, nrow = 2)
set.seed(90984)
shm <- function(t, A = 1.5, omega = 4){ # Esta es una función sinusoidal
t * A * sin(omega * t)
}
n <- 90
x <- sample(seq(0, 3, 0.02), n) # creamos una base con n observaciones
y <- shm(x) + rnorm(length(x), sd = 1)
toy <- data.frame(x, y)
library(fda)
install.packages('fda')
library(fda)
knots <- quantile(x) # nudos
base <- create.bspline.basis(
norder = 4, # polinomios cúbicos
breaks = knots # nodos en los cuartiles de x
) # base de splines
H <- eval.basis(x, base) # y = H*beta + epsilon
beta_hat <- as.vector(solve(t(H) %*% H) %*% t(H) %*% toy$y)
mu <- function(x, betas){
as.numeric(betas %*% t(eval.basis(x, base)))
}
mu_hat <- as.numeric(beta_hat %*% t(H))
sigma_hat <- sqrt(1 / n * sum((toy$y - mu_hat) ^ 2))
# simulamos 9 conjuntos de datos de acuerdo a nuestro modelo.
y_sim <- rep(mu_hat, 9) + rnorm(n * 9, sd = sigma_hat)
codigo <- sample(1:10, 10) # para poder identificar los datos
toy_null <- data.frame(x = rep(x, 10), y = c(y, y_sim),
id = rep(codigo, each = n))
ggplot(toy_null, aes(x = x, y = y)) +
geom_point(size = 0.8)  +
facet_wrap(~ id, nrow = 2)
knitr::opts_chunk$set(
comment = "#>",
collapse = TRUE,
fig.width=3, fig.height=3
)
comma <- function(x) format(x, digits = 2, big.mark = ",")
options(digits=3)
library(tidyverse)
library(gridExtra)
theme_set(theme_minimal())
se_fun_n <- function(n, p) sqrt(p * (1-p) / n)
xy <- data.frame(x = 20:220, y = seq(0, 1, 0.005))
ggplot(xy, aes(x = x, y = y)) +
stat_function(fun = se_fun_n, args = list(p = 0.7), aes(color = "p=0.7")) +
stat_function(fun = se_fun_n, args = list(p = 0.9), aes(color = "p=0.9")) +
stat_function(fun = se_fun_n, args = list(p = 0.5), aes(color = "p=0.5")) +
labs(x = "n", y = "se", color = "") +
geom_segment(x = 20, xend = 100, y = 0.05, yend = 0.05, color = "red",
alpha = 0.3, linetype = "longdash") +
geom_segment(x = 100, xend = 100, y = 0.05, yend = 0, color = "red",
alpha = 0.3, linetype = "longdash")
sim_p_hat <- function(n, p, n_sims = 1000){
sim_muestra <- rbinom(n_sims, size = n, prob = p)
se_p_hat <- sd(sim_muestra / n)
data_frame(n = n, se_p_hat = se_p_hat, p = p)
}
sims_.7 <- map_df(seq(20, 220, 10), ~sim_p_hat(n = ., p = 0.7))
sims_.5 <- map_df(seq(20, 220, 10), ~sim_p_hat(n = ., p = 0.5))
sims_.6 <- map_df(seq(20, 220, 10), ~sim_p_hat(n = ., p = 0.6))
sims_.9 <- map_df(seq(20, 220, 10), ~sim_p_hat(n = ., p = 0.9))
sims <- bind_rows(sims_.7, sims_.5, sims_.6, sims_.9)
ggplot(sims, aes(x = n, y = se_p_hat, color = factor(p), group = p)) +
geom_smooth(se = FALSE, size = 0.5) +
labs(x = "n", y = "se", color = "") +
geom_segment(x = 20, xend = 100, y = 0.05, yend = 0.05, color = "red",
alpha = 0.3, linetype = "longdash") +
geom_segment(x = 100, xend = 100, y = 0.05, yend = 0, color = "red",
alpha = 0.3, linetype = "longdash")
sim_potencia <- function(n, p, n_sims = 1000){
sim_muestra <- rbinom(n_sims, size = n, prob = p)
se_p_hat <- sd(sim_muestra / n)
acepta <- (sim_muestra / n - 1.96 * se_p_hat) > 0.5
data_frame(n = n, potencia = mean(acepta))
}
sims <- map_df(c(2, 10, 50, 80, 100, 150, 200, 300, 500), ~sim_potencia(n = ., p = 0.6))
ggplot(sims) +
geom_line(aes(x = n, y = potencia)) +
geom_hline(yintercept = 0.8, color = "red", alpha = 0.5)
library(lme4)
allvar <- read.csv("data/allvar.csv")
cd4 <- allvar %>%
filter(treatmnt == 1, !is.na(CD4PCT), baseage > 1, baseage < 5) %>%
mutate(
y = sqrt(CD4PCT),
person = newpid,
time = visage - baseage
)
ggplot(cd4, aes(x = time, y = y, group = person)) +
geom_line(alpha = 0.5)
fit_cd4 <- lmer(formula = y ~ time + (1 + time | person), cd4)
fit_cd4
cd4_sim <- function (J, K, mu.a.true = 4.8, g.0.true = -0.5, g.1.true = 0.5,
sigma.y.true = 0.7, sigma.a.true = 1.3, sigma.b.true = 0.7){
time <- rep(seq(0, 1, length = K), J) # K mediciones en el año
person <- rep (1:J, each = K)        # ids
treatment <- sample(rep(0:1, J/2))
treatment1 <- treatment[person]
# parámetros a nivel persona
a.true <- rnorm(J, mu.a.true, sigma.a.true)
b.true <- rnorm(J, g.0.true + g.1.true * treatment, sigma.b.true)
y <- rnorm (J * K, a.true[person] + b.true[person] * time, sigma.y.true)
return (data.frame(y, time, person, treatment1))
}
# 2.  Function to fit the model and calculate the power
cd4_power <- function (J, K){
fake <- cd4_sim(J, K)
lme_power <- lmer (y ~ time + time:treatment1 + (1 + time | person), data = fake)
theta_hat <- fixef(lme_power)["time:treatment1"]
theta_se <- summary(lme_power)$coefficients["time:treatment1", "Std. Error"]
theta_hat - 2 * theta_se > 0
}
cd4_rep <- function(n_sims, J, K){
rerun(n_sims, cd4_power(J, K = 7)) %>% flatten_dbl() %>% mean()
}
potencias <- map_df(c(4, 16, 60, 100, 150, 200, 225, 250, 300, 400),
~data_frame(n = ., p = cd4_rep(n_sims = 500, J = ., K = 7)))
ggplot(potencias, aes(x = n, y = p)) +
geom_hline(yintercept = 0.8, color = "red", alpha = 0.5) +
geom_line()
potencias_2 <- map_df(c(4, 16, 60, 100, 150, 200, 225, 250, 300, 400),
~data_frame(n = ., p = cd4_rep(n_sims = 200, J = ., K = 3)))
ggplot(potencias, aes(x = n, y = p)) +
geom_hline(yintercept = 0.8, color = "red", alpha = 0.5) +
geom_line() +
geom_line(data = potencias_2)
# Verosimilitud X_1,...,X_n ~ Bernoulli(theta)
L_bernoulli <- function(n, S){
function(theta){
theta ^ S * (1 - theta) ^ (n - S)
}
}
# log-verosimilitud
l_bernoulli <- function(n, S){
function(theta){
S * log(theta) + (n - S) * log(1 - theta)
}
}
xy <- data.frame(x = 0:1, y = 0:1)
verosimilitud <- ggplot(xy, aes(x = x, y = y)) +
stat_function(fun = L_bernoulli(n = 20, S = 12)) +
xlab(expression(theta)) +
ylab(expression(L(theta))) +
ggtitle("Verosimilitud (n=20, S = 12)")
log_verosimilitud <- ggplot(xy, aes(x = x, y = y)) +
stat_function(fun = l_bernoulli(n = 20, S = 12))+
xlab(expression(theta)) +
ylab(expression(l(theta))) +
ggtitle("log-verosimilitud (n=20, S = 12)")
grid.arrange(verosimilitud, log_verosimilitud, nrow = 1)
optimize(L_bernoulli(n = 20, S = 12), interval = c(0, 1), maximum = TRUE)
optimize(l_bernoulli(n = 20, S = 12), interval = c(0, 1), maximum = TRUE)
n <- 200
x <- rnorm(n, mean = 10, sd = 5)  # observaciones normales
# Paso 1: calcular mu_hat y sigma_hat
mu_hat <- mean(x)
sigma_hat <- sqrt(1 / n * sum((x - mu_hat) ^ 2))
# Pasos 2 y 3
thetaBoot <- function(){
# Simular X_1*,...X_N* con distribución N(mu_hat, sigma_hat^2)
x_boot <- rnorm(n, mean = mu_hat, sd = sigma_hat)
# Calcular mu*, sigma* y theta*
mu_boot <- mean(x_boot)
sigma_boot <- sqrt(1 / n * sum((x_boot - mu_boot) ^ 2))
sigma_boot / mu_boot # theta*
}
# Paso 4: Repetimos B = 2000 veces y estimamos el error estándar
sims_boot <- rerun(3000, thetaBoot()) %>% flatten_dbl()
sqrt(1 / 2999 * sum((sims_boot - sigma_hat/mu_hat) ^ 2))
n<-70
x<-rbernoulli(r=20)
n<-70
x<-rbernoulli(n)
20/70
n<-70
x<-rbernoulli(n,p=20/70)
theta<-mean(x)
theta
n<-70
x<-rbernoulli(n,p=(20/70))
theta<-mean(x)
# Pasos 2 y 3
thetaBoot <- function(){
# Simular X_1*,...X_N* con distribución N(mu_hat, sigma_hat^2)
x_boot <- rbernoulli(n, p=theta)
# Calcular mu*, sigma* y theta*
theta_boot <- mean(x_boot)
se_hat<-sqrt(theta*(1-theta)/n)
theta_boot # theta*
}
sims_boot <- rerun(3000, thetaBoot()) %>% flatten_dbl()
sims_boot
se<-sqrt(1 / 2999 * sum((sims_boot - theta) ^ 2))
se
theta-1.96*sd(sims_boot)
theta+1.96*sd(sims_boot)
theta
getwd()
df
setwd('/home/abraham/intro_to_ds/alumnos/Abraham_Nieto/Algas')
getwd()
algas
algas
source('toolset.R')
source('utils.R')
source('metadata.R')
source('00-load.R')
source('01-prepare.R')
str(algas)
summary(algas)
algas
summary(algas)
problems(algas)
algas[20,]
algas[problems(algas)$row,]
problematic_rows <- problems(algas)$row
algas[problematic_rows,] <- algas %>%
slice(problematic_rows) %>%
unite(col="all", -seq(1:6), sep = "/", remove=TRUE) %>%
extract(all, into=c("NO3", "NH4", "resto"),
regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=TRUE) %>%
separate(resto, into=names(algas)[9:18], sep="/", remove=TRUE)
algas[19:20,]
algas %>%
slice(problematic_rows) %>% head()
algas %>%
slice(problematic_rows) %>%
unite(col="all", -seq(1:6), sep="/", remove=T)
algas %>%
slice(problematic_rows) %>%
unite(col="all", -seq(1:6), sep="/", remove=T) %>%
extract(all, into=c("NO3", "NH4", "resto"),
regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=T)
algas <- readr::type_convert(algas)
algas
#algas <- algas %>%
#  mutate_all(funs(algas_clean))
algas
summary(algas)
glimpse(algas)
x <- as.data.frame(abs(is.na(algas))) # df es un data.frame
head(x)
# Extrae las variables que tienen algunas celdas con NAs
y <- x[which(sapply(x, sd) > 0)] #se usa sd ya que si sd>0 entonces sabemos que hay NA's
# Da la correación, un valor alto positivo significa que desaparecen juntas.
cor(y)
#-------------------------------------------------------------------------------------------
library(corrplot)
#tarea 1 library(corplot) corplot(cor(y))
#reporte de Missings:
library(Amelia)
missmap(algas, main="Missings",
col=c("yellow", "black"), legend=FALSE)
#podemos ver en esta grafica las variables con mas missings cla,cl y ahora calculamos el % de NAs que
#tiene cada variable...
mis<-colSums(is.na(algas))/nrow(algas)*100
mis
#numero de observaciones que tienen al menos 1 missing en alguna columna
nrow(algas[!complete.cases(algas),])
#observaciones con missings:
algas[!complete.cases(algas),]
#matriz de correlacion de misings...
corrplot(cor(y))
#de esta grafica podemos interpretar que los variables no3,nh4 y opo4 son missings al parejo es decir
#si una de ellas es missing las otras 2 tambien lo seran.
#tambien pasa un eefecto similar con las variables cl y chla
#-----------------------------------------------------------------------------------------------
summary(algas[-grep(colnames(algas),pattern = "^a[1-9]")]) # Nota el uso del grep
algas%>%select(-starts_with("a")) %>%
summary()
nrow(algas[!complete.cases(algas),])
algas_con_NAs <- algas[!complete.cases(algas),]
algas_con_NAs[c('max_ph', 'min_o2', 'cl', 'no3', 'nh4', 'opo4', 'po4', 'chla')]%>%print(n = 33)
# ¿Cuántos NAs hay por observación?
apply(algas, 1, function(x) sum(is.na(x)))
algas[apply(algas, 1, function(x) sum(is.na(x))) > 2,]
indices_con_NAs(algas, 0.2)
indices_con_NAs(algas, 0.8)
algas <- algas[-indices_con_NAs(algas, 0.2),]
dim(algas)
algas
algas<-imputar_valor_central(algas,{"max_ph"})
algas
algas[,-c(1:3)] %>%
cor(use="complete.obs") %>%
symnum()
print(ggplot(data=algas) +
aes(x=opo4, y=po4) +
geom_point(sape=1) + # Usamos una bolita para los puntos
geom_smooth(method=lm, se=FALSE)#+theme_hc()
)
#algas <- algas[-indices_con_NAs(algas, 0.2),]
modelo <- lm(po4 ~ opo4, data=algas)
modelo
algas<-imputar_valor_lm(algas,"po4","opo4",modelo)
algas<-imputar_por_similitud(algas,3)
class(algas)
imputar_valor_lm(algas,"po4","opo4",modelo)
algas
algas[!complete.cases(algas),]
class(algas)
modelo
imputar_valor_lm(algas,"po4","opo4",modelo)
lapply(algas[,opo4],class)
lapply(algas[,"opo4"],class)
algas<-imputar_valor_lm(algas,"po4","opo4",modelo)
source('toolset.R')
source('utils.R')
source('metadata.R')
source('00-load.R')
source('01-prepare.R')
str(algas)
summary(algas)
summary(algas)
problems(algas)
algas[20,]
algas[problems(algas)$row,]
problematic_rows <- problems(algas)$row
algas[problematic_rows,] <- algas %>%
slice(problematic_rows) %>%
unite(col="all", -seq(1:6), sep = "/", remove=TRUE) %>%
extract(all, into=c("NO3", "NH4", "resto"),
regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=TRUE) %>%
separate(resto, into=names(algas)[9:18], sep="/", remove=TRUE)
algas[19:20,]
algas %>%
slice(problematic_rows) %>% head()
algas %>%
slice(problematic_rows) %>%
unite(col="all", -seq(1:6), sep="/", remove=T)
algas %>%
slice(problematic_rows) %>%
unite(col="all", -seq(1:6), sep="/", remove=T) %>%
extract(all, into=c("NO3", "NH4", "resto"),
regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=T)
algas <- readr::type_convert(algas)
algas
#algas <- algas %>%
#  mutate_all(funs(algas_clean))
algas
summary(algas)
glimpse(algas)
x <- as.data.frame(abs(is.na(algas))) # df es un data.frame
head(x)
# Extrae las variables que tienen algunas celdas con NAs
y <- x[which(sapply(x, sd) > 0)] #se usa sd ya que si sd>0 entonces sabemos que hay NA's
# Da la correación, un valor alto positivo significa que desaparecen juntas.
cor(y)
#-------------------------------------------------------------------------------------------
library(corrplot)
#tarea 1 library(corplot) corplot(cor(y))
#reporte de Missings:
library(Amelia)
missmap(algas, main="Missings",
col=c("yellow", "black"), legend=FALSE)
#podemos ver en esta grafica las variables con mas missings cla,cl y ahora calculamos el % de NAs que
#tiene cada variable...
mis<-colSums(is.na(algas))/nrow(algas)*100
mis
#numero de observaciones que tienen al menos 1 missing en alguna columna
nrow(algas[!complete.cases(algas),])
#observaciones con missings:
algas[!complete.cases(algas),]
#matriz de correlacion de misings...
corrplot(cor(y))
#de esta grafica podemos interpretar que los variables no3,nh4 y opo4 son missings al parejo es decir
#si una de ellas es missing las otras 2 tambien lo seran.
#tambien pasa un eefecto similar con las variables cl y chla
#-----------------------------------------------------------------------------------------------
summary(algas[-grep(colnames(algas),pattern = "^a[1-9]")]) # Nota el uso del grep
algas%>%select(-starts_with("a")) %>%
summary()
nrow(algas[!complete.cases(algas),])
algas_con_NAs <- algas[!complete.cases(algas),]
algas_con_NAs[c('max_ph', 'min_o2', 'cl', 'no3', 'nh4', 'opo4', 'po4', 'chla')]%>%print(n = 33)
# ¿Cuántos NAs hay por observación?
apply(algas, 1, function(x) sum(is.na(x)))
algas[apply(algas, 1, function(x) sum(is.na(x))) > 2,]
indices_con_NAs(algas, 0.2)
indices_con_NAs(algas, 0.8)
algas <- algas[-indices_con_NAs(algas, 0.2),]
dim(algas)
#dataset$cat_with_NAs_fix <- ifelse(is.na(dataset$cat_with_NAs),
#                                   "missing",
#                                   ifelse(dataset$ccat_with_NAs == TRUE,
#                                          # o el valor que sea
#                                          "level_1",
#                                          "level_2"))
print(ggpairs(algas,lower = list(combo =wrap("facethist", binwidth=0.8))))
#----------------------------------------------------------------------------------------------------
#Ejer2
#imputar max_ph con la mediana porque su distribucion es quasi normal..
algas<-imputar_valor_central(algas,{"max_ph"})
#--------------------------------------------------------------------------------------------------
algas[,-c(1:3)] %>%
cor(use="complete.obs") %>%
symnum()
print(ggplot(data=algas) +
aes(x=opo4, y=po4) +
geom_point(sape=1) + # Usamos una bolita para los puntos
geom_smooth(method=lm, se=FALSE)#+theme_hc()
)
#algas <- algas[-indices_con_NAs(algas, 0.2),]
modelo <- lm(po4 ~ opo4, data=algas)
modelo
algas<-imputar_valor_lm(algas,"po4","opo4",modelo)
getwd()
