library(nullabor)
nd<-sample_n(diamonds,5000,replace=FALSE)
d<-lineup(null_permute("depth"),nd)
set.seed(3456543)
library(nullabor)
nd<-sample_n(diamonds,5000,replace=FALSE)
d<-lineup(null_permute("depth"),nd)
ggplot(data=d,aes(x=depth,y=carat))+geom_jitter()+facet_wrap(~.sample)
library(nullabor)
library(lattice)
data(singer)
# creo base de datos singer-gender donde Sopranos y Alto -> F
singer_g <- singer %>%
mutate(
gender = case_when(
voice.part %in% c("Soprano 1", "Soprano 2", "Alto 1", "Alto 2") ~ "F",
TRUE ~ "M"),
height = 2.54 * height) %>%
dplyr::select(gender, height)
head(singer_g)
ggplot(singer_g, aes(x = gender, y = height)) +
geom_jitter(position = position_jitter(width = 0.1, height = 1))
paths <- dir("data/2008-2009", pattern = "LAL", full.names = TRUE)
basket <- purrr::set_names(paths, paths) %>%
map_df(read_csv)
basket_LA <- basket %>%
filter(team == "LAL", type == "3pt", !is.na(x), !is.na(y)) %>% # datos Lakers
mutate(
x = x + runif(length(x), -0.5, 0.5),
y = y + runif(length(y), -0.5, 0.5),
r = sqrt((x - 25) ^ 2 + y ^ 2),  # distancia a canasta
angle = atan2(y, x - 25)) %>%  # ángulo
filter(r > 20 & r < 39) %>% # lanzamientos en el rango típico
dplyr::select(x, y, r, angle)
# guardar datos
write.table(basket_LA, file = "data/basket_LA.csv", sep = ",")
glimpse(basket_LA)
basket_null <- lineup(null_lm(r ~ poly(angle, 2)), basket_LA, n = 10)
ggplot(basket_null, aes(x = angle * 180 / pi, y = r)) +
geom_point(alpha = 0.5, size = 0.8) +
scale_x_continuous("Angle (degrees)",
breaks = c(0, 45, 90, 135, 180),
limits = c(0, 180)) +
facet_wrap(~ .sample, nrow = 2)
fit <- lm(r ~ poly(angle, 2), data = basket_LA)
n <- 10
# matriz de covariables
X <- model.matrix(r ~ poly(angle, 2), basket_LA)
n_obs <- nrow(X)
# 9 simulaciones del modelo, cada una del mismo tamaño que los datos
sims <- rep(X %*% coef(fit), n - 1) + rnorm((n - 1) * n_obs, mean = 0, sd = 1)
# creo una base de datos que incluya datos originales
basket_LA_3 <- data.frame(
angle = rep(basket_LA$angle, n), # el valor de x (ángulo) para cada panel
r = c(basket_LA$r, sims),  # distancias simuladas y observadas
id = rep(sample(1:n, size = n), each = n_obs)) # id aleatorio para esconder los datos
ggplot(basket_LA_3, aes(x = angle * 180 / pi, y = r)) +
geom_point(alpha = 0.5, size = 0.8) +
scale_x_continuous("Angle (degrees)",
breaks = c(0, 45, 90, 135, 180),
limits = c(0, 180)) +
facet_wrap(~ id, nrow = 2)
basket_LA$resid <- resid(fit)
basket_null_2 <- lineup(
null_dist("resid", "normal", list(mean = 0, sd = 1)),
basket_LA, n = 10)
ggplot(basket_null_2, aes(x = angle * 180 / pi, y = resid)) +
geom_point(alpha = 0.6, size = 0.8) +
scale_x_continuous("Angle (degrees)",
breaks = c(0, 45, 90, 135, 180),
limits = c(0, 180)) +
facet_wrap(~ .sample, nrow = 2)
set.seed(90984)
shm <- function(t, A = 1.5, omega = 4){ # Esta es una función sinusoidal
t * A * sin(omega * t)
}
n <- 90
x <- sample(seq(0, 3, 0.02), n) # creamos una base con n observaciones
y <- shm(x) + rnorm(length(x), sd = 1)
toy <- data.frame(x, y)
library(fda)
install.packages('fda')
library(fda)
knots <- quantile(x) # nudos
base <- create.bspline.basis(
norder = 4, # polinomios cúbicos
breaks = knots # nodos en los cuartiles de x
) # base de splines
H <- eval.basis(x, base) # y = H*beta + epsilon
beta_hat <- as.vector(solve(t(H) %*% H) %*% t(H) %*% toy$y)
mu <- function(x, betas){
as.numeric(betas %*% t(eval.basis(x, base)))
}
mu_hat <- as.numeric(beta_hat %*% t(H))
sigma_hat <- sqrt(1 / n * sum((toy$y - mu_hat) ^ 2))
# simulamos 9 conjuntos de datos de acuerdo a nuestro modelo.
y_sim <- rep(mu_hat, 9) + rnorm(n * 9, sd = sigma_hat)
codigo <- sample(1:10, 10) # para poder identificar los datos
toy_null <- data.frame(x = rep(x, 10), y = c(y, y_sim),
id = rep(codigo, each = n))
ggplot(toy_null, aes(x = x, y = y)) +
geom_point(size = 0.8)  +
facet_wrap(~ id, nrow = 2)
knitr::opts_chunk$set(
comment = "#>",
collapse = TRUE,
fig.width=3, fig.height=3
)
comma <- function(x) format(x, digits = 2, big.mark = ",")
options(digits=3)
library(tidyverse)
library(gridExtra)
theme_set(theme_minimal())
se_fun_n <- function(n, p) sqrt(p * (1-p) / n)
xy <- data.frame(x = 20:220, y = seq(0, 1, 0.005))
ggplot(xy, aes(x = x, y = y)) +
stat_function(fun = se_fun_n, args = list(p = 0.7), aes(color = "p=0.7")) +
stat_function(fun = se_fun_n, args = list(p = 0.9), aes(color = "p=0.9")) +
stat_function(fun = se_fun_n, args = list(p = 0.5), aes(color = "p=0.5")) +
labs(x = "n", y = "se", color = "") +
geom_segment(x = 20, xend = 100, y = 0.05, yend = 0.05, color = "red",
alpha = 0.3, linetype = "longdash") +
geom_segment(x = 100, xend = 100, y = 0.05, yend = 0, color = "red",
alpha = 0.3, linetype = "longdash")
sim_p_hat <- function(n, p, n_sims = 1000){
sim_muestra <- rbinom(n_sims, size = n, prob = p)
se_p_hat <- sd(sim_muestra / n)
data_frame(n = n, se_p_hat = se_p_hat, p = p)
}
sims_.7 <- map_df(seq(20, 220, 10), ~sim_p_hat(n = ., p = 0.7))
sims_.5 <- map_df(seq(20, 220, 10), ~sim_p_hat(n = ., p = 0.5))
sims_.6 <- map_df(seq(20, 220, 10), ~sim_p_hat(n = ., p = 0.6))
sims_.9 <- map_df(seq(20, 220, 10), ~sim_p_hat(n = ., p = 0.9))
sims <- bind_rows(sims_.7, sims_.5, sims_.6, sims_.9)
ggplot(sims, aes(x = n, y = se_p_hat, color = factor(p), group = p)) +
geom_smooth(se = FALSE, size = 0.5) +
labs(x = "n", y = "se", color = "") +
geom_segment(x = 20, xend = 100, y = 0.05, yend = 0.05, color = "red",
alpha = 0.3, linetype = "longdash") +
geom_segment(x = 100, xend = 100, y = 0.05, yend = 0, color = "red",
alpha = 0.3, linetype = "longdash")
sim_potencia <- function(n, p, n_sims = 1000){
sim_muestra <- rbinom(n_sims, size = n, prob = p)
se_p_hat <- sd(sim_muestra / n)
acepta <- (sim_muestra / n - 1.96 * se_p_hat) > 0.5
data_frame(n = n, potencia = mean(acepta))
}
sims <- map_df(c(2, 10, 50, 80, 100, 150, 200, 300, 500), ~sim_potencia(n = ., p = 0.6))
ggplot(sims) +
geom_line(aes(x = n, y = potencia)) +
geom_hline(yintercept = 0.8, color = "red", alpha = 0.5)
library(lme4)
allvar <- read.csv("data/allvar.csv")
cd4 <- allvar %>%
filter(treatmnt == 1, !is.na(CD4PCT), baseage > 1, baseage < 5) %>%
mutate(
y = sqrt(CD4PCT),
person = newpid,
time = visage - baseage
)
ggplot(cd4, aes(x = time, y = y, group = person)) +
geom_line(alpha = 0.5)
fit_cd4 <- lmer(formula = y ~ time + (1 + time | person), cd4)
fit_cd4
cd4_sim <- function (J, K, mu.a.true = 4.8, g.0.true = -0.5, g.1.true = 0.5,
sigma.y.true = 0.7, sigma.a.true = 1.3, sigma.b.true = 0.7){
time <- rep(seq(0, 1, length = K), J) # K mediciones en el año
person <- rep (1:J, each = K)        # ids
treatment <- sample(rep(0:1, J/2))
treatment1 <- treatment[person]
# parámetros a nivel persona
a.true <- rnorm(J, mu.a.true, sigma.a.true)
b.true <- rnorm(J, g.0.true + g.1.true * treatment, sigma.b.true)
y <- rnorm (J * K, a.true[person] + b.true[person] * time, sigma.y.true)
return (data.frame(y, time, person, treatment1))
}
# 2.  Function to fit the model and calculate the power
cd4_power <- function (J, K){
fake <- cd4_sim(J, K)
lme_power <- lmer (y ~ time + time:treatment1 + (1 + time | person), data = fake)
theta_hat <- fixef(lme_power)["time:treatment1"]
theta_se <- summary(lme_power)$coefficients["time:treatment1", "Std. Error"]
theta_hat - 2 * theta_se > 0
}
cd4_rep <- function(n_sims, J, K){
rerun(n_sims, cd4_power(J, K = 7)) %>% flatten_dbl() %>% mean()
}
potencias <- map_df(c(4, 16, 60, 100, 150, 200, 225, 250, 300, 400),
~data_frame(n = ., p = cd4_rep(n_sims = 500, J = ., K = 7)))
ggplot(potencias, aes(x = n, y = p)) +
geom_hline(yintercept = 0.8, color = "red", alpha = 0.5) +
geom_line()
potencias_2 <- map_df(c(4, 16, 60, 100, 150, 200, 225, 250, 300, 400),
~data_frame(n = ., p = cd4_rep(n_sims = 200, J = ., K = 3)))
ggplot(potencias, aes(x = n, y = p)) +
geom_hline(yintercept = 0.8, color = "red", alpha = 0.5) +
geom_line() +
geom_line(data = potencias_2)
# Verosimilitud X_1,...,X_n ~ Bernoulli(theta)
L_bernoulli <- function(n, S){
function(theta){
theta ^ S * (1 - theta) ^ (n - S)
}
}
# log-verosimilitud
l_bernoulli <- function(n, S){
function(theta){
S * log(theta) + (n - S) * log(1 - theta)
}
}
xy <- data.frame(x = 0:1, y = 0:1)
verosimilitud <- ggplot(xy, aes(x = x, y = y)) +
stat_function(fun = L_bernoulli(n = 20, S = 12)) +
xlab(expression(theta)) +
ylab(expression(L(theta))) +
ggtitle("Verosimilitud (n=20, S = 12)")
log_verosimilitud <- ggplot(xy, aes(x = x, y = y)) +
stat_function(fun = l_bernoulli(n = 20, S = 12))+
xlab(expression(theta)) +
ylab(expression(l(theta))) +
ggtitle("log-verosimilitud (n=20, S = 12)")
grid.arrange(verosimilitud, log_verosimilitud, nrow = 1)
optimize(L_bernoulli(n = 20, S = 12), interval = c(0, 1), maximum = TRUE)
optimize(l_bernoulli(n = 20, S = 12), interval = c(0, 1), maximum = TRUE)
n <- 200
x <- rnorm(n, mean = 10, sd = 5)  # observaciones normales
# Paso 1: calcular mu_hat y sigma_hat
mu_hat <- mean(x)
sigma_hat <- sqrt(1 / n * sum((x - mu_hat) ^ 2))
# Pasos 2 y 3
thetaBoot <- function(){
# Simular X_1*,...X_N* con distribución N(mu_hat, sigma_hat^2)
x_boot <- rnorm(n, mean = mu_hat, sd = sigma_hat)
# Calcular mu*, sigma* y theta*
mu_boot <- mean(x_boot)
sigma_boot <- sqrt(1 / n * sum((x_boot - mu_boot) ^ 2))
sigma_boot / mu_boot # theta*
}
# Paso 4: Repetimos B = 2000 veces y estimamos el error estándar
sims_boot <- rerun(3000, thetaBoot()) %>% flatten_dbl()
sqrt(1 / 2999 * sum((sims_boot - sigma_hat/mu_hat) ^ 2))
n<-70
x<-rbernoulli(r=20)
n<-70
x<-rbernoulli(n)
20/70
n<-70
x<-rbernoulli(n,p=20/70)
theta<-mean(x)
theta
n<-70
x<-rbernoulli(n,p=(20/70))
theta<-mean(x)
# Pasos 2 y 3
thetaBoot <- function(){
# Simular X_1*,...X_N* con distribución N(mu_hat, sigma_hat^2)
x_boot <- rbernoulli(n, p=theta)
# Calcular mu*, sigma* y theta*
theta_boot <- mean(x_boot)
se_hat<-sqrt(theta*(1-theta)/n)
theta_boot # theta*
}
sims_boot <- rerun(3000, thetaBoot()) %>% flatten_dbl()
sims_boot
se<-sqrt(1 / 2999 * sum((sims_boot - theta) ^ 2))
se
theta-1.96*sd(sims_boot)
theta+1.96*sd(sims_boot)
theta
lapply(paquetes, instalar)
instalar <- function(paquete) {
if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}
}
paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest',
'ggplot2', 'stringr', 'ggthemes', 'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
'GGally', 'readODS', 'readxl', "RSQLite")
lapply(paquetes, instalar)
getwd
getwd()
titanic_path <- '/intro_to_ds/data/Titanic/titanic.ods'
ds_names <- ods_sheets(titanic_path)
ds_names
instalar <- function(paquete) {
if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}
}
paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest',
'ggplot2', 'stringr', 'ggthemes', 'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
'GGally', 'readODS', 'readxl', "RSQLite")
lapply(paquetes, instalar)
titanic_path <- '/intro_to_ds/data/Titanic/titanic.ods'
titanic_path <- '/home/abraham/intro_to_ds/data/Titanic/titanic.ods'
ds_names <- ods_sheets(titanic_path)
ds_names
instalar <- function(paquete) {
if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}
}
paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest',
'ggplot2', 'stringr', 'ggthemes', 'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
'GGally', 'readODS', 'readxl', "RSQLite")
lapply(paquetes, instalar)
install.packages('ggthemes')
instalar <- function(paquete) {
if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}
}
paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest',
'ggplot2', 'stringr', #'ggthemes',
'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
'GGally', 'readODS', 'readxl', "RSQLite")
lapply(paquetes, instalar)
installed.packages('readxl')
install.packages('readxl')
titanic_path <- '/home/abraham/intro_to_ds/data/Titanic/titanic.ods'
ds_names <- ods_sheets(titanic_path)
ds_names
library(readxl)
titanic_path <- '/home/abraham/intro_to_ds/data/Titanic/titanic.ods'
ds_names <- ods_sheets(titanic_path)
ds_names
ds_names <- ods_sheets(titanic_path)
install.packages('readODS')
titanic_path <- '/home/abraham/intro_to_ds/data/Titanic/titanic.ods'
ds_names <- ods_sheets(titanic_path)
ds_names
library(readODS)
titanic_path <- '/home/abraham/intro_to_ds/data/Titanic/titanic.ods'
ds_names <- ods_sheets(titanic_path)
ds_names
clean_sheet_name <- function(sheet_name) {
str_replace_all(str_replace_all(string=sheet_name, pattern=" ", replace="_"), pattern="'", replace="") %>%
str_to_lower()
}
sapply(ds_names, clean_sheet_name)
save_sheet <- function(sheet_name) {
file_name <-  paste0("../data/Titanic/", clean_sheet_name(sheet_name), ".rds")
saveRDS(object = read_ods(titanic_path, sheet = sheet_name), file = file_name)
}
lapply(ods_sheets(titanic_path), save_sheet)
save_sheet <- function(sheet_name) {
file_name <-  paste0("/home/abraham/intro_to_ds/data/Titanic/", clean_sheet_name(sheet_name), ".rds")
saveRDS(object = read_ods(titanic_path, sheet = sheet_name), file = file_name)
}
lapply(ods_sheets(titanic_path), save_sheet)
rm(list=ls())
rds_files <- dir("../data/Titanic/", pattern = "*.rds", full.names = TRUE)
#lapply te devolverá las cosas en un lista... una lista de dataframes :)
ds <- lapply(rds_files, read_rds)
class(ds)
class(ds[[1]])
#cuantos dataframes contiene esta lista?
length(ds)
#basename elimina todo el path del nombre excepto la última parte (se quedará con la extensión del archivo!), ?basename
names(ds) <- lapply(rds_files, basename)
names(ds)
rm(list=ls())
rds_files <- dir("/home/abraham/intro_to_ds/data/Titanic/", pattern = "*.rds", full.names = TRUE)
#lapply te devolverá las cosas en un lista... una lista de dataframes :)
ds <- lapply(rds_files, read_rds)
class(ds)
class(ds[[1]])
#cuantos dataframes contiene esta lista?
length(ds)
#basename elimina todo el path del nombre excepto la última parte (se quedará con la extensión del archivo!), ?basename
names(ds) <- lapply(rds_files, basename)
names(ds)
#veamos qué nombres tiene cada dataframe
lapply(ds, names)
#si quisieramos obtener los conjuntos de nombres únicos
lapply(ds, names) %>% unique()
lapply(ds, head)
ds <- ds[-which(lapply(lapply(ds, names), length) == 2)]
num_cols <- lapply((lapply(ds, names)), length) %>% unlist() %>% min()
num_cols
lapply(ds, str)
ds <- lapply(ds, function(x) lapply(x, as.character))
#verifiquemos
lapply(ds, str)
#bind_rows es como rbind solo que optimizado por Hadley Wickham :)
titanic <- bind_rows(ds)[, 1:num_cols]
names(titanic) <- str_replace_all(names(titanic), "/| ", "_") %>%
str_to_lower()
names(titanic)
titanic <- tbl_df(titanic)
titanic
titanic <- titanic %>%
separate(name, into=c("last_name", "name"), sep=",", extra="drop") %>%
separate(fare, into=c("pounds", "shillings", "pence"), sep=" ", extra="drop") %>%
separate(age, into=c("age", "units"), sep=2, extra="drop") %>%
mutate(sex=ifelse(grepl("Miss|Mrs|Mme.|Lady|Doña|Ms", name), 'F',
ifelse(grepl("Mr|Sir|Sig|Dr|Master|Captain|Major|Rev.|Colonel|Fr|Don.", name), 'M', NA))) %>%
mutate(boat_location=ifelse(as.integer(boat) %in% c(9:16), 'Popa',
ifelse(boat %in% c(LETTERS[1:4]) | as.integer(boat) %in% c(1:8), 'Proa', NA))) %>%
mutate(age=ifelse(units == "m", 1, as.integer(age))) %>%
mutate(survived=!is.na(boat)) %>%
dplyr::select(-c(shillings, pence, body, units)) %>%
mutate(pounds=str_replace(pounds, "£", "") %>% as.integer()) %>%
mutate(class_dept=as.factor(class_dept), group=as.factor(group), ship=as.factor(ship),
joined=as.factor(joined), job=as.factor(job), boat=as.factor(boat),
sex=as.factor(sex), boat_location=as.factor(boat_location))
summary(titanic)
titanic <- titanic %>% mutate(age = ifelse(age <= 18, "infante",
ifelse(age > 65, "adulto mayor",
"adulto")))
#verifiquemos
titanic
ggplot(titanic, aes(pounds)) +
geom_histogram(binwidth = 30, na.rm=T) +
theme_bw()
titanic <- titanic %>%
group_by(ticket) %>%
mutate(pounds_per_ticket = round(pounds/n())) %>%
ungroup()
titanic
titanic %>% filter(class_dept %in% c('1st Class', '2nd Class', '3rd Class')) %>%
ggplot(aes(pounds_per_ticket)) +
geom_histogram(binwidth = 10) +
facet_grid(class_dept~., scales = "free_y") +
theme_bw()
titanic %>%
group_by(boat_location) %>%
summarise(n=n())
titanic %>%
group_by(boat) %>%
summarise(n=n()) %>%
arrange(desc(n))
sqlite3
library(RSQLite)
install.packages('RSQLite')
library(RSQLite)
berka_db <- src_sqlite(path="~/Documents/itam/introduction_to_ds/intro_to_ds/data/berka/berka.raw",
create=FALSE)
db_list_tables(berka_db$con)
setwd("/home/abraham/tarea_mineria/examen1")
prior <- function(mu, tau){
function(theta){
return(dnorm(theta, mean=mu, sd=1/tau))
}
}
prior(150,15)
prior <- function(mu, tau){
function(theta){
return(dnorm(theta, mean=mu, sd=1/tau))
}
}
prior(150,15)
prior <- function(mu, tau){
function(theta){
dnorm(theta, mean=mu, sd=1/tau)
}
}
prior(150,15)
prior <- function(mu, tau){
function(theta){
dnorm(theta, mean=mu, sd=1/tau)
}
}
miprior<-prior(150,15)
2*pi
2*pi*20**2
20**2
# S: sum x_i, S2: sum x_i^2, N: número obs.
likeNorm <- function(S, S2, N){
function(theta){
(1/((2*pi*20**2)**(N/2)))*exp((-1/(2*20**2))(s2-2*theta*S+N*theta**2))
}
}
milike<-likeNorm(13000,1700000,100)
postRelProb <- function(theta){
mi_like(theta) * mi_prior(theta)
}
milike(2)
miprior()<-prior(150,15)
miprior(theta)<-prior(150,15)
miprior<-prior(150,15)
milike(34)
dnorm(2,150,1/15)
dnorm(45,150,1/15)
dnorm(1000,150,1/15)
dnorm(150,1/15)
dbeta(2,1,1)
dbeta(.2,1,1)
dbeta(.12,1,1)
dbeta(.123,1,1)
dbeta(.323,1,1)
dnorm(.3,150,1/15)
dnorm(.5,150,1/15)
dbeta(.39,2,2)
dbeta(.5,2,2)
dnorm(100,150,1/15)
?dnorm
x<-c(.025,.975,1)
dnorm(x,150,1/15)
dnorm(1,150,1/15)
dnorm(3456,150,1/15)
t<-seq(media0-3.5126*sd0, media0+3.5126*sd0, length.out=1000)
t<-seq(150-3.5126*(1/15), 150+3.5126*(1/15), length.out=1000)
t
dnorm(t,150,1/15)
mi_prior<-prior(150,15)
mi_prior(t)
likeNorm(t)
